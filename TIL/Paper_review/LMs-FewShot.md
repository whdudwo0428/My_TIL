## Language Models are Few-Shot Learners

#### 2025-09-07

### Problem Statement

* 기존 언어 모델들은 대규모 사전 학습 이후에도 새로운 작업을 수행하려면 **fine-tuning**이나 **task-specific 아키텍처 수정**이 필요했다.
* 실제 응용에서는 다양한 작업에 빠르게 적응할 수 있는 **범용성**이 중요하다.
* 따라서, **대규모 언어 모델이 사전 학습만으로 새로운 작업을 수행할 수 있는가** 그리고 **적은 예시(few-shot)만으로 학습 없이도 일반화가 가능한가**라는 질문이 제기되었다.

### Solution Approach

* OpenAI 연구진(Brown et al., 2020)은 **GPT-3** (175B 파라미터)를 개발하고, **in-context learning** 능력을 평가했다.
* 핵심 아이디어는 모델을 재학습하지 않고, **프롬프트(prompt)에 몇 개의 예시를 제공하는 것만으로 새로운 작업 수행이 가능하다**는 점을 검증하는 것이다.
* 이를 통해 **zero-shot, one-shot, few-shot 학습 능력**을 체계적으로 분석했다.

### Methodology Details

* 모델 규모: 175B 파라미터 Transformer (GPT-2의 확장판).
* 실험 설정:

  * zero-shot: 작업 설명만 제공.
  * one-shot: 예시 하나 제공.
  * few-shot: 예시 여러 개 제공 (보통 10\~100개).
* 평가 영역: 번역, 질의응답, 산술, 상식 추론, 텍스트 생성 등 다양한 NLP 벤치마크.
* 결과:

  * few-shot 설정에서 GPT-3는 기존 최고 성능 모델들과 경쟁할 만한 성과를 보였다.
  * zero-shot과 one-shot 상황에서도 놀라운 일반화 능력을 보였다.
  * 하지만 복잡한 추론, 수학 계산, 사실적 정합성에서는 여전히 한계가 뚜렷했다.
* 한계:

  * 데이터셋 편향으로 인한 사회적 편향 문제.
  * 장기 추론과 사실적 정확성 부족으로 인한 hallucination.
  * 학습과 추론에 필요한 막대한 컴퓨트 자원.

### Recap

이 논문은 **GPT-3**를 통해 대규모 언어 모델이 사전 학습만으로도 **범용적 능력**을 획득한다는 점을 보여주었다.
핵심 발견은 **in-context learning**으로, 프롬프트에 몇 개의 예시만 제공해도 모델이 새로운 작업을 수행할 수 있다는 것이다.
이는 기존 fine-tuning 중심 접근에서 **prompt 기반 패러다임**으로의 전환을 이끌었으며, 이후 ChatGPT와 같은 대화형 모델 개발로 이어졌다.
