## Scaling Laws for Autoregressive Generative Models

#### 2025-09-07

### Problem Statement

* 기존의 **Scaling Laws for Neural Language Models** (**Kaplan et al., 2020**)는 언어 모델에 한정되어 있었다.
* 그러나 생성 모델(generative models)은 언어뿐 아니라 이미지, 오디오, 멀티모달 데이터 등 다양한 도메인에 걸쳐 사용된다.
* 따라서, **자원(모델 크기, 데이터 크기, 컴퓨트)과 성능의 관계가 언어 외 다른 생성 모델에도 보편적으로 적용되는지** 검증할 필요가 있었다.
* 특히, **오토리그레시브(autoregressive) 구조**를 갖는 생성 모델 전반에 대해 scaling law가 성립하는지 탐구하는 것이 핵심 목표였다.

### Solution Approach

* 저자들은 **언어 모델, 이미지 모델, 기타 오토리그레시브 생성 모델**을 대상으로 실험을 확장했다.
* 핵심 가설: 손실 감소 패턴이 언어 모델뿐 아니라 \*\*모든 오토리그레시브 생성 모델에서도 파워 법칙(power-law scaling)\*\*을 따른다.
* 즉, scaling law는 특정 데이터 타입에 종속된 것이 아니라 \*\*모델 구조(오토리그레시브)\*\*에 의해 일반적으로 성립한다.

### Methodology Details

* 다양한 도메인의 모델 학습 실험을 수행:

  * **언어 모델** (GPT류)
  * **이미지 모델** (PixelCNN, Image GPT 등)
  * **멀티모달 모델** (텍스트-이미지 등 일부)
* 측정 변수:

  * 모델 파라미터 수
  * 학습 데이터(토큰/픽셀 시퀀스) 크기
  * 컴퓨트 (FLOPs)
* 결과:

  * 언어와 이미지 모두에서 손실 감소가 파워 법칙을 따른다.
  * 최적 자원 배분 전략 역시 유사하게 성립한다.
  * 단, 도메인별 \*\*손실 함수의 기저(base loss floor)\*\*가 다르게 나타났다. 즉, 언어는 엔트로피 기반(perplexity), 이미지는 픽셀 레벨 likelihood 기반이라 손실 스케일 차이가 존재.
* 결론:

  * **Scaling Laws는 언어 모델에 한정된 현상이 아니며, 오토리그레시브 생성 모델 전반에 적용 가능**하다.
  * 모델 크기와 데이터 크기의 균형적 확장이 중요하다는 점도 동일하게 관찰됨.

### Recap

이 논문은 **Scaling Laws**를 언어 모델에서 일반 **오토리그레시브 생성 모델**로 확장해 보편성을 검증한 연구다.
핵심 결과는 언어, 이미지 등 서로 다른 도메인에서도 **손실이 파워 법칙을 따른다**는 점이며, 주어진 자원을 최적으로 분배하는 방식 역시 언어 모델과 동일하게 적용할 수 있다는 것이다.
즉, Scaling Laws는 특정 데이터 도메인에 국한되지 않고, 오토리그레시브 생성 패러다임 전반에 걸쳐 관찰되는 보편적 법칙임을 보여주었다.
