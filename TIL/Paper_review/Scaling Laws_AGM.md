## Scaling Laws for Autoregressive Generative Models

#### 2025-09-07

### Problem Statement

* 기존 연구인 *Scaling Laws for Neural Language Models* (Kaplan et al., 2020)은 언어 모델에 국한된 결과였다.
* 그러나 생성 모델은 텍스트뿐 아니라 이미지, 오디오, 멀티모달 등 다양한 도메인에서 사용된다.
* 따라서 자원(모델 파라미터 수, 데이터 크기, 연산량)과 성능 사이의 관계가 언어 외의 오토리그레시브 생성 모델에도 일반적으로 적용되는지 확인할 필요가 있었다.
* 특히, **오토리그레시브 구조**라는 공통 특성을 기반으로 보편적 scaling law가 성립하는지 검증하는 것이 목표였다.

### Solution Approach

* 언어 모델 외에도 **이미지 생성 모델**과 다른 오토리그레시브 생성 모델을 대상으로 실험을 확장했다.
* 가설: 손실 감소 패턴은 데이터 도메인에 종속된 것이 아니라, 오토리그레시브 구조에 의해 결정된다.
* 즉, 언어 모델에서 발견된 파워 법칙 관계가 이미지, 멀티모달 모델에도 성립할 것이라 제안했다.

### Methodology Details

* 모델 범위:

  * 언어 모델 (Transformer 기반 GPT류)
  * 이미지 모델 (PixelCNN, Image GPT 등)
  * 일부 멀티모달 오토리그레시브 모델
* 변수:

  * 모델 파라미터 수
  * 데이터 크기 (토큰 수, 픽셀 시퀀스 길이)
  * 연산량 (FLOPs)
* 결과:

  * 언어와 이미지 모두에서 손실 감소가 **파워 법칙**을 따른다는 사실이 관찰되었다.
  * 최적 자원 배분 전략 역시 유사하게 성립했다.
  * 다만 손실 함수의 기준선(loss floor)은 도메인별로 달랐다. 예를 들어, 언어 모델은 엔트로피 기반(perplexity), 이미지 모델은 픽셀 likelihood 기반으로 측정되어 절대값 비교에는 차이가 있었다.
* 결론:

  * scaling law는 언어 모델에 국한되지 않고, **오토리그레시브 생성 패러다임 전반**에 적용된다.
  * 따라서 모델 크기와 데이터 크기를 균형 있게 확장해야 한다는 원칙이 도메인에 관계없이 타당하다.

### Recap

이 논문은 언어 모델에서 발견된 scaling law가 이미지와 멀티모달 오토리그레시브 생성 모델에도 일반적으로 적용됨을 보여주었다.
핵심 발견은 **손실이 파워 법칙을 따른다**는 점이며, **모델 크기와 데이터 크기를 균형적으로 확장하는 전략이 최적**이라는 것이다.
즉, scaling law는 특정 데이터 도메인에 한정된 것이 아니라, 오토리그레시브 생성 구조 자체에 의해 나타나는 보편적 법칙임을 입증했다.
